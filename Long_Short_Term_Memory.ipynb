{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elysethulin/PRACTICE/blob/master/Long_Short_Term_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWDFXbE-uAVb"
      },
      "source": [
        "# Long Short Term Memory\n",
        "\n",
        "In this notebook, we use a Long Short Term Memory Neural Network (LSTM) model to classify time series data. This model is built using the `tensorflow` and `keras` libraries, similar to the networks we constructed yesterday.\n",
        "\n",
        "The notebook is split into the following sections:\n",
        "1. Investigate the data\n",
        "2. Construct an LSTM model\n",
        "3. Evaluate the model\n",
        "\n",
        "Author: Joshua Pickard (jpic@umich.edu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGqghDdFuAVc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM  \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "import glob\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrot1nRJuAVe"
      },
      "source": [
        "## Human Activity Recognition Using Smartphones Data Set\n",
        "\n",
        "We will be using machine learning to preform Human Activity Recognition (HAR) for the following 3 activities:\n",
        "\n",
        "1. Running\n",
        "2. Stationary\n",
        "3. Walking\n",
        "\n",
        "To do this, we will analyze 3 signals collected from people as they preformed these activities. Participants wore a Samsung Galaxy S II. Using its embedded accelerometer and gyroscope, 3-axial linear acceleration and 3-axial angular velocity were recorded at a constant rate of 50Hz.\n",
        "\n",
        "The acceleration measurements taken for each dimension (x, y, z) will be the input data our models consider. Each sample of data contains all 3 acceleration measurements for 1000 points in time.\n",
        "\n",
        "For more information on the dataset, please see: https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below cell, a `.zip` file is downloaded from the given url, and each file is unpacked. The files come in the form of `csv` files, where each column is an acceleration measurement in 1 dimension, each row is a point in time, and each file is one sample."
      ],
      "metadata": {
        "id": "4nEqecTJLiy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://github.com/STMicroelectronics/stm32ai/raw/master/AI_resources/HAR/dataset.zip\n",
        "!unzip -n dataset.zip"
      ],
      "metadata": {
        "id": "KmcHad-8D5md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sI3haGJuAVf"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "The data has been downloaded to the `dataset/` directory and can be viewed with the file icon on the left hand side of the page. The following cell of code loads the data to `X` and `y` variables representing a sample of recordings and an action label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "072wWg9-uAVf"
      },
      "outputs": [],
      "source": [
        "# Load data into memory\n",
        "labels = ['stationary', 'walking', 'running']\n",
        "x_recordings = []\n",
        "y_recordings = []\n",
        "recordings_filenames = []\n",
        "for i, label in enumerate(labels):\n",
        "    filenames = glob.glob('dataset/' + label + '/*.csv')\n",
        "    for filename in filenames:\n",
        "        data = np.loadtxt(filename, delimiter=',')\n",
        "        x_recordings.append(data)\n",
        "        y_recordings.append(i)\n",
        "        recordings_filenames.append(filename)\n",
        "\n",
        "x_recordings = np.array(x_recordings).reshape(len(x_recordings), -1, 3)\n",
        "y_recordings = np.array(y_recordings)\n",
        "\n",
        "print(x_recordings.shape)\n",
        "print(y_recordings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output above, we see the shape of `x_recordings`, the data we downloaded, is `(92, 1000, 3)`. There are 3 measurements taken at any time, 1000 time points per sample, and 92 samples in all."
      ],
      "metadata": {
        "id": "s-Mz8DbKL1wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Data"
      ],
      "metadata": {
        "id": "WHXtcDnjM6ZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below generates a few plots to visualize what different sample look like when plotted over time. It appears as if each activity has its own set of trends in terms of how it appears. If we were doing feature selction or signals processing, we may be itnerested in smoothing or otherwise modifying this dataset to remove noise in the data."
      ],
      "metadata": {
        "id": "v7r6xwQ4MQq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot some captures\n",
        "unique_rands = random.sample(range(len(x_recordings)), 10)\n",
        "plt.figure(figsize=(18, 10))\n",
        "for i, n in enumerate(unique_rands):\n",
        "    plt.subplot(5, 2, i + 1)\n",
        "    plt.margins(x=0, y=-0.25)\n",
        "    plt.plot(x_recordings[n])\n",
        "    plt.ylim(-4000, 4000)  # 4000 mg acc. range\n",
        "    plt.title(recordings_filenames[n].split('/')[-1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "665d7khxE9TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64CInu2kuAVj"
      },
      "source": [
        "## Class Balance\n",
        "As we have seen, it is important to understand the class balance of a data set. In the case of a HAR classification problem, we are interested in knowing how many examples of each type of activity we have in the data set. In the following cell, we explore this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX9YMMrCuAVk"
      },
      "outputs": [],
      "source": [
        "# convert the numpy array into a dataframe\n",
        "df = pd.DataFrame(y_recordings)\n",
        "counts = df.groupby(0).size()\n",
        "counts = counts.values\n",
        "\n",
        "# summarize\n",
        "print('Training Data Set:')\n",
        "for i in range(len(counts)):\n",
        "    percent = counts[i] / len(df) * 100\n",
        "    print('Class=%d, total=%d, percentage=%.3f' % (i+1, counts[i], percent))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded and visualized a few realizations of our data, we partition the data into training and validation datasets."
      ],
      "metadata": {
        "id": "5EN-RWLVFSwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(x_recordings, y_recordings)\n",
        "\n",
        "# Verify that the data split correctly\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "VuXbhovbFZnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Test your Model\n",
        "\n",
        "In this next section, we build and evaluate a LSTM model on this time series data."
      ],
      "metadata": {
        "id": "KM66l2h2H9G_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_YQnyfHuAVm"
      },
      "source": [
        "### Reformat the Labels\n",
        "\n",
        "In the next cell, we reformat the labels to be compatable with the `Sequential` model from `tensorflow`. This reformats the labels into One Hot Encodings, which is a different format than we used when generating the plots above.\n",
        "\n",
        "The labels for walking, sitting, and running are numeric values, but there is no ordinal relationship between these activities. As a result, we transform the labels to a representation that does not assign an order to each activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_FWIvfDuAVm"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "XEfm1RuJG2z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing the LSTM Model\n",
        "\n",
        "This syntax is nearly identical to the synatx used to construct CNN or Feedforward networks. The key difference is the first layer is `LSTM` to make use of this time series."
      ],
      "metadata": {
        "id": "t8JhKR8kNXAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "model.add(LSTM(100, input_shape=(1000, 3)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# TODO: add more hidden layers\n",
        "# fill in the activations as sigmoid, softmax, relu, or tanh\n",
        "model.add(Dense(10, activation='sigmoid'))\n",
        "\n",
        "# Do not change this line\n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['AUC'])"
      ],
      "metadata": {
        "id": "A1OgR4hfIEee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "UQu3iHrKIWG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_activations=True)"
      ],
      "metadata": {
        "id": "bf_EyY7COCZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit network\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, epochs=epochs)"
      ],
      "metadata": {
        "id": "Ud7CKvX0IVKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['auc'])\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Accuracy During Training')"
      ],
      "metadata": {
        "id": "jwM6oaQ9Tl-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can evaluate this trained model on the test data set."
      ],
      "metadata": {
        "id": "9ESK8pScOamu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, AUC = model.evaluate(X_test, y_test)\n",
        "print('AUC', AUC) "
      ],
      "metadata": {
        "id": "H07Ty1jeJNF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Long Short Term Memory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}