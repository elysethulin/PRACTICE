{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elysethulin/PRACTICE/blob/master/Introduction_to_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8f948c",
      "metadata": {
        "id": "9f8f948c"
      },
      "source": [
        "# Introduction to Clustering\n",
        "\n",
        "This notebook contains the code for several methods of clustering data. It uses previous libraries we have seen, `NumPy` and `Matplotlib`, as well as `sklearn` a popular library for machine learning in python. It executes several methods for clustering data that we have already seen in previous sections, and it introduces several new methods.\n",
        "\n",
        "Author: Joshua Pickard (jpic@umich.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries\n",
        "1. NumPy: Numerical computing\n",
        "2. Matplotlib: plotting\n",
        "3. sklearn: machine learning library\n",
        "4. Pandas: library for handling data"
      ],
      "metadata": {
        "id": "1yUgWBHC2Ehx"
      },
      "id": "1yUgWBHC2Ehx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d0dc73",
      "metadata": {
        "id": "76d0dc73"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import cluster\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad117087",
      "metadata": {
        "id": "ad117087"
      },
      "source": [
        "# Data\n",
        "\n",
        "There are 3 main ways to work with data in CoLab notebooks:\n",
        "1. Load data from a file online\n",
        "2. Load data from a file you upload from your computer\n",
        "3. Generate your own data\n",
        "\n",
        "`Pandas` is a python library for loading and working with data, typically in the the form of a `.csv` type file, into a Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53f5442",
      "metadata": {
        "scrolled": true,
        "id": "f53f5442"
      },
      "outputs": [],
      "source": [
        "# Load data saved online\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/Jpickard1/MIDASBioMedBootCamp/main/data/clustering/2D%20data.csv')\n",
        "X1 = df.values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load your own data, upload a `.csv` file to the `Files` tab on the left. The data can be loaded in similarly to the above cell with the following line of code."
      ],
      "metadata": {
        "id": "-6PuYUEWagxl"
      },
      "id": "-6PuYUEWagxl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from your machine\n",
        "df = pd.read_csv('simulated_data.csv')\n",
        "X1 = df.values"
      ],
      "metadata": {
        "id": "aAW0CqdBYrIj"
      },
      "id": "aAW0CqdBYrIj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate simulated data\n",
        "\n",
        "The `sklearn` library provides a number of functions to generate simulated data. "
      ],
      "metadata": {
        "id": "wYjacGqPfRqT"
      },
      "id": "wYjacGqPfRqT"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# TODO: Fill in the parameters to generate data\n",
        "X1, true_labels = make_blobs(n_samples=   , n_features=    , centers =    )"
      ],
      "metadata": {
        "id": "-uLAMlgSfi5o"
      },
      "id": "-uLAMlgSfi5o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pandas\n",
        "\n",
        "`.head` and `print(dataframe)` are 2 comands to view a few lines of your data set and make sure they are loaded correctly."
      ],
      "metadata": {
        "id": "b_9E9JTGaSCh"
      },
      "id": "b_9E9JTGaSCh"
    },
    {
      "cell_type": "code",
      "source": [
        "df.head"
      ],
      "metadata": {
        "id": "oa-wDwAzZ8UT"
      },
      "id": "oa-wDwAzZ8UT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "Y-wfipg_aNgk"
      },
      "id": "Y-wfipg_aNgk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "52c24aca",
      "metadata": {
        "id": "52c24aca"
      },
      "source": [
        "### Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9adb4f",
      "metadata": {
        "id": "9c9adb4f"
      },
      "outputs": [],
      "source": [
        "plt.scatter(    ,   )\n",
        "# plt.ylim(   ,   )\n",
        "# plt.xlim(   ,   )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KMeans Clustering\n",
        "\n",
        "In the previous lecture, we discussed the KMeans clustering algorithm. In the cells below, we will implement this algorithm from scratch.\n",
        "\n",
        "The first cell is preimplemented with a distance function. It calculates the `euclidean` distance between a point and all points in the data matrix. Euclidean distance is the standard measure we typically use to measure distance in the real world, but we could exchange this distance function for other distance measures.\n",
        "\n",
        "The next cell contains the `KMeans_clustering` function. Once the variables are initialized, the function iteratively assigns points to clusters and updates the cluster center."
      ],
      "metadata": {
        "id": "YLTR3l8MvFTx"
      },
      "id": "YLTR3l8MvFTx"
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean(point, data):\n",
        "    \"\"\"\n",
        "    Euclidean distance between point & data.\n",
        "    Point has dimensions (m,), data has dimensions (n,m), and output will be of size (n,).\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.sum((point - data)**2, axis=1))"
      ],
      "metadata": {
        "id": "NzN_QHvaUbHD"
      },
      "id": "NzN_QHvaUbHD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KMeans_clustering(Xtrain, k=2, max_iter = 5, plot_on=True):\n",
        "\n",
        "  # Initialize a labels vector that the function will return\n",
        "  labels = np.zeros((len(Xtrain), 1))\n",
        "\n",
        "  # Randomly select centroid start points\n",
        "  centroid_idxs = np.array(random.sample(range(len(Xtrain)), k))\n",
        "  centroids = Xtrain[centroid_idxs]\n",
        "\n",
        "  # Iterate, adjusting centroids until converged or until passed max_iter\n",
        "  iteration = 0\n",
        "  prev_centroids = None\n",
        "\n",
        "  # ITERATE: Condition to stop the algorithm\n",
        "  while (iteration == 0 or np.linalg.norm(np.array(centroids) - np.array(prev_centroids)) > 1e-5) and iteration < max_iter:\n",
        "    # Sort each datapoint, assigning to nearest centroid\n",
        "    clusters = [[] for _ in range(k)]\n",
        "\n",
        "    # ASSIGNMENT each data point x to a cluster\n",
        "    for i in range(len(Xtrain)):\n",
        "      x = Xtrain[i]\n",
        "      dists = euclidean(x, centroids)\n",
        "      centroid_idx = np.argmin(dists)\n",
        "      clusters[centroid_idx].append(x)\n",
        "      labels[i] = centroid_idx\n",
        "    \n",
        "    # Save previous centroids\n",
        "    prev_centroids = centroids\n",
        "    \n",
        "    # AVERAGING: Calculate new centroids\n",
        "    centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
        "    # Set centroid as an element of the data\n",
        "    for i in range(len(centroids)):\n",
        "      dists = euclidean(centroids[i], clusters[i])\n",
        "      centroid_idx = np.argmin(dists)\n",
        "      centroids[i] = clusters[i][centroid_idx]\n",
        "\n",
        "    # Plot iteration\n",
        "    if plot_on:\n",
        "      plt.figure(iteration)\n",
        "      plt.scatter(Xtrain[:, 0], Xtrain[:,1], c=labels)\n",
        "      plt.title(\"Iteration: \" + str(iteration))\n",
        "      plt.show()\n",
        "      print(np.linalg.norm(np.array(centroids) - np.array(prev_centroids)) )\n",
        "\n",
        "    # Increase iteration number\n",
        "    iteration += 1\n",
        "    \n",
        "  return labels\n"
      ],
      "metadata": {
        "id": "K6kAf14cTk1D"
      },
      "id": "K6kAf14cTk1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = KMeans_clustering(X1, k=4, max_iter = 5)"
      ],
      "metadata": {
        "id": "WXiyYEgFSaMQ"
      },
      "id": "WXiyYEgFSaMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4198873f",
      "metadata": {
        "id": "4198873f"
      },
      "source": [
        "## Sklearn Clustering\n",
        "\n",
        "The Sklearn library proides a number of off the shelf clustering algorithms. Below are examples of 3 different ways to cluster data poins: \n",
        "1. Kmeans Clustering\n",
        "2. Spectral clustering\n",
        "3. Agglomerative or Hierarchical Clustering\n",
        "\n",
        "We will first explore these 3 algorithms on the data set visualized above, and 2 other simple data sets below. The goal is to gain an understanding of the effectiveness and trade offs between the algorithms. Then we will move to another data set, which contains 15 dimensional data. There the goal will be to explore the Sklearn library and use it to practice clustering the data set using different algorithms. More information on the different clustering algorithms provided in the library can be found at:\n",
        "https://scikit-learn.org/stable/modules/clustering.html#k-means"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8884ccf0",
      "metadata": {
        "id": "8884ccf0"
      },
      "source": [
        "### Kmeans Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d1c497",
      "metadata": {
        "id": "11d1c497"
      },
      "outputs": [],
      "source": [
        "kmeans = cluster.KMeans(n_clusters=6)\n",
        "kmeans.fit(X1)\n",
        "y_predictions = kmeans.predict(X1)\n",
        "plt.scatter(X1[:, 0], X1[:, 1], c=y_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2852e3",
      "metadata": {
        "id": "cc2852e3"
      },
      "source": [
        "### Spectral Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa2da2d",
      "metadata": {
        "id": "7aa2da2d"
      },
      "outputs": [],
      "source": [
        "spectral = cluster.SpectralClustering(n_clusters = 3)\n",
        "spectral.fit(X1)\n",
        "y_predictions = spectral.fit_predict(X1)\n",
        "plt.scatter(X1[:, 0], X1[:, 1], c=y_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5711b1b0",
      "metadata": {
        "id": "5711b1b0"
      },
      "source": [
        "### Agglomerative Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd23cbcc",
      "metadata": {
        "id": "cd23cbcc"
      },
      "outputs": [],
      "source": [
        "agglomerative = sklearn.cluster.AgglomerativeClustering(n_clusters = 5)\n",
        "agglomerative.fit(X1)\n",
        "y_predictions = agglomerative.labels_\n",
        "plt.scatter(X1[:, 0], X1[:, 1], c=y_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc8a249d",
      "metadata": {
        "id": "fc8a249d"
      },
      "source": [
        "## Additional Data Sets\n",
        "\n",
        "Below we have simulated data sets that highlight the shortcomings of some of the above algorithms. These data sets are interesting because while it is easy to visualize them and identify the clusters, the overlapping nature in them makes it challenging for the algorithms to correctly separate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0aee8c",
      "metadata": {
        "id": "fe0aee8c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Jpickard1/MIDASBioMedBootCamp/main/data/clustering/X1.csv')\n",
        "X1 = df.values\n",
        "plt.scatter(X1[:, 0], X1[:, 1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ec6e50",
      "metadata": {
        "id": "c1ec6e50"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Jpickard1/MIDASBioMedBootCamp/main/data/clustering/X2.csv')\n",
        "X2 = df.values\n",
        "plt.scatter(X2[:, 0], X2[:, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e993ff4d",
      "metadata": {
        "id": "e993ff4d"
      },
      "source": [
        "While there are clearly 2 distinct groups in both data sets above, separating them can becomes an interesting problem. In the cell below, choose a data set and one of the above clustering algorithsm (or another clustering method from the sklearn library) and see how it runs on these data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc40d77d",
      "metadata": {
        "id": "dc40d77d"
      },
      "outputs": [],
      "source": [
        "# TODO: Set the data set to X1 or X2\n",
        "data = \n",
        "# Fill in the clustering algorithm here. Make sure to give the argument (n_clusters=2)\n",
        "model = cluster.SpectralClustering(n_clusters=2)\n",
        "# Have the model execute the clustering algorithm\n",
        "model.fit(data)\n",
        "# Get the cluster labels of each point\n",
        "y_predictions = model.labels_\n",
        "# Create a scatter plot based on the data\n",
        "plt.scatter(data[:, 0], data[:, 1], c=y_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c51c927",
      "metadata": {
        "id": "4c51c927"
      },
      "source": [
        "A great overview of clustering algorithms is given at: https://scikit-learn.org/stable/modules/clustering.html In the image below, it shows how a number of clustering algorithms will preform on different types of data sets. In the cell above, we did a similar experiment using data sets similar to the top 2 rows of this diagram.\n",
        "\n",
        "Note that the run time for each clustering below is given in the bottom right corner."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46ce881",
      "metadata": {
        "id": "d46ce881"
      },
      "source": [
        "![Clustering Algorithms](https://github.com/Jpickard1/MIDASBioMedBootCamp/blob/main/Session_5/imgs/clustering_algorithm_comparisons.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca33a0d",
      "metadata": {
        "id": "3ca33a0d"
      },
      "source": [
        "# Higher Dimensional Clustering\n",
        "\n",
        "Now that we have used a few different clustering algorithms on 2D data, we will apply the same methologies to 15 dimensional data. We will apply the same steps as above to analyze this data set:\n",
        "1. Load the data\n",
        "2. View data to verify it loaded correctly\n",
        "3. Visualize the data\n",
        "4. Apply clustering algorithms\n",
        "5. Analyze the output of each clustering algorithm\n",
        "\n",
        "Once it is loaded and we analize the number of clusters, we should revisit the link above and explore the other clustering options provided in Sklearn.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/clustering.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bbb0574",
      "metadata": {
        "id": "6bbb0574"
      },
      "source": [
        "## Load Data Set 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973aa6ee",
      "metadata": {
        "scrolled": true,
        "id": "973aa6ee"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Jpickard1/MIDASBioMedBootCamp/main/Session_5/set_4.csv')\n",
        "X2 = df.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the data set\n",
        "print(df.head)"
      ],
      "metadata": {
        "id": "0734BJKWeSku"
      },
      "id": "0734BJKWeSku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c0228224",
      "metadata": {
        "id": "c0228224"
      },
      "source": [
        "## Visualize the Data\n",
        "\n",
        "Visualize your data set similar to how we plotted the earlier data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d83f0a",
      "metadata": {
        "id": "c9d83f0a"
      },
      "outputs": [],
      "source": [
        "# TODO: Creat a scatter plot of the data\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(X2[:, 0], X2[:, 2], X2[:, 3])\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the 2 dimensional data used in the first section of this notebook, each mode of the data was plotted on its own access, and we could see the entire data set without projecting it to a lower dimension. To view a 15 dimension data set, we must project it to a lower dimension to visualize it. In the above cell, try plotting different indices of the data set to how it impacts the shape or number of clusters you observe in the data."
      ],
      "metadata": {
        "id": "Y4sfkIgFg0Vj"
      },
      "id": "Y4sfkIgFg0Vj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster the data\n",
        "\n",
        "Based on your visualizations and practicing clustering the earlier data sets, cluster this new data and visualize the clusters."
      ],
      "metadata": {
        "id": "09nW6KwPiOOC"
      },
      "id": "09nW6KwPiOOC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster the data\n",
        "data = X2\n",
        "model = cluster.KMeans(n_clusters=3)\n",
        "# Have the model execute the clustering algorithm\n",
        "model.fit(data)\n",
        "# Get the cluster labels of each point\n",
        "y_predictions = model.labels_"
      ],
      "metadata": {
        "id": "YPnAMHONiM_v"
      },
      "id": "YPnAMHONiM_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the clustering\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(X2[:, 0], X2[:, 2], X2[:, 3], c=y_predictions)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')"
      ],
      "metadata": {
        "id": "k-eaBygZipTq"
      },
      "id": "k-eaBygZipTq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When analyzing the quality of clusters, it may be helpful to again visualize multiple projections of the data into 3D space."
      ],
      "metadata": {
        "id": "M5ZKQXSAi3oD"
      },
      "id": "M5ZKQXSAi3oD"
    },
    {
      "cell_type": "markdown",
      "id": "6664dcec",
      "metadata": {
        "id": "6664dcec"
      },
      "source": [
        "# Analyze the Number of Clusters\n",
        "\n",
        "By now, we have probably noticed that plotting this data (clustered or unclustered) is entirely uninformative for the number of clusters and how well the clusters were formed. This is a problem of working with high dimensional data: There is no good way to visualize it.\n",
        "\n",
        "Here, this creates 2 main problems:\n",
        "1. We don't know how many clusters are in the data\n",
        "2. Even if we did, we can't determine if the clusters we make are good\n",
        "\n",
        "Rather than analyzing the clusters visually, we can compute a score for how well data is clustered according to a particular algorithm.\n",
        "\n",
        "## Clustering Measures\n",
        "\n",
        "There are a many measures that tell us how good our clusters are. These measures are motivated by some properties we would like to see in our clusters:\n",
        "\n",
        "1. small within cluster distance\n",
        "2. high between cluster distance\n",
        "\n",
        "The Davies-Bouldin score will evaluate the quality of a set of clusters motivated by these 2 main properties. To use this measure to evaluate the clustering quality, we can cluster the data with various numbers of clusters, compute the score, and select the optimal value. The aim is to minimize the Davies-Bouldin score.\n",
        "\n",
        "Additional clustering scores reference: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.cluster\n",
        "\n",
        "Davies-Bouldin code reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html\n",
        "\n",
        "Davies-Bouldin theorey reference: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4766909."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04112031",
      "metadata": {
        "id": "04112031"
      },
      "outputs": [],
      "source": [
        "number_of_clusters = np.zeros(16)\n",
        "davies_bouldin_score = np.zeros(16)\n",
        "for i in range(2,len(number_of_clusters),1):\n",
        "    number_of_clusters[i] = i\n",
        "    kmeans = sklearn.cluster.KMeans(n_clusters=i)\n",
        "    kmeans.fit(X2)\n",
        "    labels = kmeans.labels_\n",
        "    davies_bouldin_score[i] = sklearn.metrics.davies_bouldin_score(X2, labels)\n",
        "    \n",
        "plt.plot(number_of_clusters, davies_bouldin_score)\n",
        "plt.xlim(2,16)\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Davis-Bouldin Score\")\n",
        "plt.title(\"Optimal Number of Clusters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f67de79",
      "metadata": {
        "id": "0f67de79"
      },
      "source": [
        "Using the optimal number of clusters identified above, now cluster this data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d38249b",
      "metadata": {
        "id": "7d38249b"
      },
      "outputs": [],
      "source": [
        "kmeans = sklearn.cluster.KMeans(n_clusters=7)\n",
        "kmeans.fit(X2)\n",
        "labels = kmeans.labels_\n",
        "plt.scatter(X2[:, 0], X2[:, 1], labels)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Introduction to Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}